# 1stNeuralNetProject
Investigating the accuracy of a Neural Network on predicting Race


Facial Recognition (and some bonus fun) Project

Ariel Simmons

I. Introduction
Background
	Facial recognition, one of the most remarkable emerging technologies in modern society, has been applied in multiple fields such as unlocking smartphones, identifying missing people, and verifying identities for safer transactions. The rapid development of facial recognition is due to people’s awareness of the significance of the facial image. These facial images are related to one’s personal information, such as age, gender, race, and even cultural background. Facial recognition technologies draw from all of these characteristics to function. Therefore, this topic deserves further research and exploration so as to play a more important role in society while protecting the individuals at stake. 

Project Idea
	Realizing the significance of facial recognition, I planned to explore how it could be applied to mugshots to better identify criminals. I were eager to see how the application of unstructured data analytics skills could be helpful to predict one’s characteristics based on the facial image through image analysis techniques. In this project, I built a predictive model and specified people’s race as the target label. I also wanted to expand on our analysis and look at the crimes associated with the images separately. From an image analysis standpoint, I hoped to understand potential challenges in facial recognition and find areas where it seems to fail. In the realm of text analysis, I hoped to extract meaningful information such as crime frequency, association by race, and association with other crimes. I acknowledged that there are several ethical arguments in this field and I would discuss them as well.

II. Dataset Overview
Data Identification
	The dataset I used for this project was released by Indiana Department of Correction (IDOC) and was downloaded from Kaggle (https://www.kaggle.com/elliotp/idoc-mugshots). The dataset included 70,008 front-facing images and 70,008 corresponding side-facing images. Each image pair belonged to a unique prisoner. Beyond facial images, the more pertinent segment of our dataset was the 69,827 samples that had related demographic information, including sex, height, weight, hair color, eye color, race, and type of offense. This dataset was perfect for our project because it had comprehensive information about each facial image for us to build the model, and I could apply textual analysis on the description of offense to find something interesting as well. 

Data Preparation & Cleaning
	For the code to work conveniently in our image analysis, I uploaded all front images and side images into two separate folders in the Google Drive. In this way, I could access all images through the cloud in either Google Colaboratory or Jupyter Notebook. Given that the original dataset was complete and tidy, I didn’t spend much time on data cleaning except removing all samples with missing values in the race column, assigning race (Black, White, Hispanic) as a numeric value (0, 1, 2) to run properly with our models, and standardizing the images to be scaled to the same size. I also double-checked the dataset to ensure that all images and personal information were exactly one-to-one based on ID numbers. 
	In terms of our textual analysis, there was also a bit of data preparation for the models I ended up running. Since images were irrelevant, I focused on the .csv labels file that corresponded with the dataset, then took the offenses column and converted it into one singular string. I then tokenized the string of offenses and created a count vectorization/vocab from the words. After ensuring the above steps were completed, the dataset was ready to go. 

III. Application of Unstructured Analytics Methods
Packages Used
	In this project, I used multiple python libraries and packages to help us to run through the whole unstructured analytics process. I imported basic packages such as os, pandas, random, and matplotlib to read and import all images, modify the dataset, and generate term frequencies plots.  I also imported Python Imaging Library (PIL) given that I mainly dealt with images in this project. For both image and textual analysis, I utilized packages from sklearn and tensorflow to divide the dataset and train and test models.  

Image Analysis
	To build a model to predict one’s race based on his or her mugshots, I chose to apply the neural network model, which was suitable and efficient for image classification. Considering that the original dataset was pretty large I selected 3% of the dataset (2109 observations) as the sample to work on to avoid any issues of timing out when running our code. I divided the sample dataset into a training dataset (65%) and a testing dataset (35%). Building the neural network model, I added a flatten layer and a dense layer and specified Adam as the optimizer and loss and accuracy as metrics to evaluate the performance of the model. After training the model with 5 epochs, the model reached 72.63% accuracy with 9.437 loss on the training dataset. Then I applied the model to the testing dataset and obtained 78.76% accuracy with 11.0153 loss as the result. The higher accuracy on the testing data indicated that the model didn’t overfit on the training dataset, which was good, but the loss increased slightly as the cost. 

Textual Analysis
	Our group conducted three different types of textual analysis, the first being topic modeling to see if I could find a grouping various types of crimes that would be similar. For this, I made five topics and simply printed out the top five words for each topic to see if I could gather any insights based on the offenses that were grouped together. I also conducted sentiment analysis using VADER, splitting the offenses into training and testing data (65% and 35% respectively) in order to see if I could gather any significant information about the attitude associated with the crimes in the dataset and whether or not there would be any “positive” associations, and which crimes were deemed more negative. Finally, I conducted another sentiment analysis via naive bayes text analysis to come up with a model that could potentially identify if an individual is a sex offender, as well as correctly identifying the race. For the naive bayes text analysis, our testing and training data was based off the offenses and the label of race and whether or not the prisoner was a sex offender.

IV. Insights & Recommendations
Image Analysis Insights
	From the image analysis, I realized that it was not hard to build a neural network model that could predict one’s race based on photos at relatively high accuracy. However, improving the model further to pursue a higher accuracy and a balance between the accuracy and loss would be a challenging task. One reason making the model hard to be enhanced was the neural network’s “black box” nature. The neural network’s low interpretability prevented us from understanding the driving factors behind the model. I couldn’t tell whether the model made predictions according to the person’s skin color, eye color, or any facial features. Therefore, it would be challenging to apply a focused improvement on the model. What’s more, after printing out all predicted labels, I found that the model didn’t recognize any sample in the testing data as Hispanic due to the limitation of the training data. Neural networks require much more data than traditional models, and the unbalanced training dataset may lead to the bad performance of the neural network to identify minority labels. I learned how powerful the neural network could be in the image classification field, while on the other hand, our project reflected that the neural network was not the most appropriate algorithm for small datasets.

Textual Analysis Insights
	Unfortunately, our topic modeling did not produce the meaningful results I had hoped for. There were no clear similarities between the top offenses within topics, and some of the returned results had ambiguous significance (e.g. ‘2nd’, ‘15’) that could be relevant to a multitude of different crimes. As for the two different sentiment analysis models, the naive bayes yielded a higher accuracy than VADER, but both proved to have valuable potential. The VADER sentiment analysis was hindered by the jargon used in the original dataset, as there are many abbreviations for certain offenses that may not have been picked up by the rule-based classification the model relies on. However, it did point out that some crimes had a more “positive” attitude than others (e.g. offenses pertaining to murder had a more negative sentiment than drug offenses). As for the naive bayes, this model did not need much training and was quite helpful in getting the interpretation I sought out to predict if someone’s race and whether or not they are a sex offenders. The model was very accurate with regards to sex offender status (96%), while less accurate on predicting race (57%). I see this lower accuracy in a positive light, however, as it provides some relief in the objectivity of our model by showing how offenses are not race-specific.

Reflections & Recommendations
	For reflections on our project and recommendations for future research or studies on facial recognition, I mainly wanted to discuss four parts. First, the development of the facial recognition model required a comprehensive dataset: a larger one with more diverse samples. Besides, adding more images instead of merely front and side images could be helpful. Secondly, to train an advanced model, I could add more layers, and I could train the model to make predictions based on specific factors, such as facial features. I could even teach the computer some existing knowledge about how to identify race instead of letting the model classify samples by itself. Next, the dataset I used were mugshots, which were clear to be identified. If the model can be trained to predict personal characteristics precisely through unclear photos, it would be more practical as it can be used to identify potential criminals or find missing people. In terms of naive bayes text analysis, our most promising model, the results could be improved upon if I looked at other characteristics besides sex offender status and race. Last but not least, facial recognition could cause ethical dilemmas, such as race discrimination caused by biased training dataset and personal privacy issues. What’s more, facial recognition might be abused or misused by some private organizations for financial or even illegal purposes. Overall, facial recognition is a double-edged sword and I need to have clear answers to those ethical questions before moving forward in this field. 

V. Conclusion
	Throughout the project, I explored how neural networks could be applied to image classification and how machine learning could help facial recognition continue to evolve. I also learned how topic modelings and sentiment analysis can be used to categorize and exact key points from textual data, and even to predict the attitude and relevant information of the text. I not only experienced the power and benefits brought by these unstructured analytics methods but also understood their drawbacks and limitations. The project on facial recognition let us realize that unstructured data analysis is important to society and full of potentials that deserve people’s further explorations.  


